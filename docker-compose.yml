# ============================================
# Digital Human Clone Dashboard - Docker Compose
# ============================================
#
# Usage:
#   Build:   docker-compose build
#   Run:     docker-compose up
#   Stop:    docker-compose down
#   Logs:    docker-compose logs -f
#
# GPU Support:
#   Requires NVIDIA Container Toolkit installed on host
#   Install: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
#
# ============================================

services:
  digital-human:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: digital-human-dashboard

    # ─────────────────────────────────────────
    # GPU Access Configuration
    # ─────────────────────────────────────────
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [ gpu ]

    # Environment variables
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      # Whisper model size (tiny, base, small, medium, large-v2, large-v3)
      - WHISPER_MODEL_SIZE=medium
      # Llama model path (relative to /app inside container)
      - LLAMA_MODEL_PATH=/app/models/Llama-3-8B-Instruct-v0.1.Q5_K_M.gguf
      # Coqui TTS agreement
      - COQUI_TOS_AGREED=1
      # Gradio settings
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=7860

    # ─────────────────────────────────────────
    # Port Mapping
    # ─────────────────────────────────────────
    ports:
      - "7860:7860"

    # ─────────────────────────────────────────
    # Volume Mounts (Persistent Data)
    # ─────────────────────────────────────────
    volumes:
      # Mount local models directory (Llama GGUF files)
      - ./models:/app/models:ro

      # Mount data directory (profiles, transcripts, vector DBs)
      # This is read-write so the app can save data
      - ./data:/app/data:rw

      # Mount output directory for generated audio
      - ./output:/app/output:rw

      # Optional: Mount temp_audio for temporary files
      - ./temp_audio:/app/temp_audio:rw

      # Mount HuggingFace cache to avoid re-downloading models
      - huggingface_cache:/root/.cache/huggingface

    # ─────────────────────────────────────────
    # Container Settings
    # ─────────────────────────────────────────
    restart: unless-stopped

    # Increase shared memory for PyTorch/CUDA
    shm_size: '4gb'

    # Allow CUDA debugging
    ulimits:
      memlock:
        soft: -1
        hard: -1
      stack:
        soft: 67108864
        hard: 67108864

    # Health check
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:7860/" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

# ─────────────────────────────────────────────
# Named Volumes
# ─────────────────────────────────────────────
volumes:
  # Persistent cache for HuggingFace models (Whisper, TTS, etc.)
  huggingface_cache:
    driver: local
